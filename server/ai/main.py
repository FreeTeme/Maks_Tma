import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import requests
import time
from typing import Dict, List, Tuple, Optional, Union
import hashlib
from pathlib import Path

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –≤–µ—Å–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å–≤–µ—á–µ–π
W_BODY = 0.65
W_VOL = 0.18
W_UP = 0.085
W_LOW = 0.085

# –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å–≤–µ—á–µ–π
IDENTICAL_THRESHOLD = 0.09
SIMILAR_THRESHOLD = 0.18

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä —Å # –≤ —Ñ–æ—Ä–º–∞—Ç Binance
SYMBOL_MAP = {
    'ETH#USDT': 'ETHUSDT',
    'BTC#USDT': 'BTCUSDT', 
    'XRP#USDT': 'XRPUSDT',
    'SOL#USDT': 'SOLUSDT',
    'BNB#USDT': 'BNBUSDT',
    'TRX#USDT': 'TRXUSDT',
    'ADA#USDT': 'ADAUSDT',
    'SUI#USDT': 'SUIUSDT',
    'LINK#USDT': 'LINKUSDT',
    'LTC#USDT': 'LTCUSDT',
    'TON#USDT': 'TONUSDT'
}

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã Binance
TIMEFRAME_MAP = {
    '1h': '1h', 
    '4h': '4h',
    '1d': '1d',
    '1w': '1w'
}

def normalize_symbol(symbol: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–∏–º–≤–æ–ª –¥–ª—è Binance API"""
    # –ï—Å–ª–∏ —Å–∏–º–≤–æ–ª —Å–æ–¥–µ—Ä–∂–∏—Ç #, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –µ–≥–æ
    normalized = SYMBOL_MAP.get(symbol, symbol.replace('#', ''))
    
    # –î–æ–±–∞–≤–ª—è–µ–º USDT –µ—Å–ª–∏ —Å–∏–º–≤–æ–ª —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–∞—Ä)
    if len(normalized) <= 6 and not normalized.endswith('USDT'):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–∏–º–≤–æ–ª –≤ —Å–ø–∏—Å–∫–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö
        supported_symbols = ['BTC', 'ETH', 'XRP', 'SOL', 'BNB', 'TRX', 'ADA', 'SUI', 'LINK', 'LTC', 'TON']
        if normalized in supported_symbols:
            normalized += 'USDT'
    
    return normalized

# –ï–¥–∏–Ω—ã–π –∫—ç—à –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
CACHE_DIR = Path("pattern_cache")
CACHE_DIR.mkdir(exist_ok=True)

def get_cache_key(*args):
    """–°–æ–∑–¥–∞–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∞ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤"""
    key_str = "_".join(str(arg) for arg in args)
    return hashlib.md5(key_str.encode()).hexdigest()

def save_to_cache(key, data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à"""
    try:
        cache_file = CACHE_DIR / f"{key}.pkl"
        pd.to_pickle(data, cache_file)
    except:
        pass

def load_from_cache(key):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞"""
    try:
        cache_file = CACHE_DIR / f"{key}.pkl"
        if cache_file.exists():
            return pd.read_pickle(cache_file)
    except:
        pass
    return None

def fetch_binance_ohlcv_fast(start_date: str, end_date: str, timeframe: str = '1d', symbol: str = 'BTCUSDT') -> pd.DataFrame:
    """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö OHLCV —Å Binance API –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞—Ä—ã"""
    normalized_symbol = normalize_symbol(symbol)
    cache_key = get_cache_key("ohlcv", normalized_symbol, timeframe, start_date, end_date)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached_data = load_from_cache(cache_key)
    if cached_data is not None:
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∞—Ç—ã –±–µ–∑ —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å–∞
        cached_data = cached_data.copy()
        cached_data['date'] = pd.to_datetime(cached_data['date']).dt.tz_localize(None)
        return cached_data
    
    base_url = 'https://api.binance.com/api/v3/klines'
    interval = TIMEFRAME_MAP.get(timeframe, '1d')
    limit = 1000
    
    start_ts = int(pd.to_datetime(start_date).timestamp() * 1000)
    end_ts = int(pd.to_datetime(end_date).timestamp() * 1000)
    
    all_rows = []
    current_ts = start_ts
    
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö {normalized_symbol} {timeframe} —Å {start_date} –ø–æ {end_date}")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –¥–ª—è –Ω–µ–¥–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç
    if timeframe == '1w':
        limit = 2000
    
    while current_ts <= end_ts:
        params = {
            'symbol': normalized_symbol,
            'interval': interval,
            'startTime': current_ts,
            'endTime': end_ts,
            'limit': limit
        }
        
        try:
            response = requests.get(base_url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            if not data:
                break
                
            for kline in data:
                all_rows.append({
                    'date': pd.to_datetime(kline[0], unit='ms'),
                    'open': float(kline[1]),
                    'high': float(kline[2]),
                    'low': float(kline[3]),
                    'close': float(kline[4]),
                    'volume': float(kline[5]),
                    'timeframe': timeframe,
                    'symbol': normalized_symbol
                })
            
            if len(data) < limit:
                break
                
            current_ts = data[-1][6] + 1
            time.sleep(0.1)
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {normalized_symbol}: {e}")
            break
    
    if not all_rows:
        return pd.DataFrame()
        
    df = pd.DataFrame(all_rows)
    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)
    df = df.sort_values('date').drop_duplicates(subset=['date']).reset_index(drop=True)
    
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    save_to_cache(cache_key, df)
    
    return df

def get_ohlcv_data(timeframe: str = '1d', symbol: str = 'BTCUSDT') -> pd.DataFrame:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö OHLCV —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è —á–∞—Å–æ–≤—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤"""
    normalized_symbol = normalize_symbol(symbol)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    cache_key = get_cache_key("full_data", normalized_symbol, timeframe)
    cached_data = load_from_cache(cache_key)
    if cached_data is not None:
        cached_data = cached_data.copy()
        cached_data['date'] = pd.to_datetime(cached_data['date']).dt.tz_localize(None)
        
        # –î–ª—è —á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        if timeframe == '1h':
            earliest_date = cached_data['date'].min()
            required_start = datetime.now() - timedelta(days=800)  # ~2.2 –≥–æ–¥–∞ –¥–ª—è —á–∞—Å–æ–≤—ã—Ö
            if earliest_date > required_start:
                print(f"‚ö†Ô∏è –í –∫—ç—à–µ —á–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ —Å {earliest_date.date()}, –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º...")
                cache_file = CACHE_DIR / f"{cache_key}.pkl"
                if cache_file.exists():
                    cache_file.unlink()
            else:
                return cached_data
        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å 2017 –≥–æ–¥–∞
            earliest_date = cached_data['date'].min()
            if earliest_date > pd.to_datetime('2018-01-01'):
                print(f"‚ö†Ô∏è –í –∫—ç—à–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ —Å {earliest_date.date()}, –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º —Å 2017 –≥–æ–¥–∞...")
                cache_file = CACHE_DIR / f"{cache_key}.pkl"
                if cache_file.exists():
                    cache_file.unlink()
            else:
                return cached_data
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
    end_date = (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')
    
    if timeframe == '1h':
        # –î–ª—è —á–∞—Å–æ–≤—ã—Ö –≥—Ä—É–∑–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2.5 –≥–æ–¥–∞ (~22K —Å–≤–µ—á–µ–π)
        start_date = (datetime.now() - timedelta(days=900)).strftime('%Y-%m-%d')
        print(f"üì• –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {normalized_symbol} —Å {start_date}")
    elif timeframe == '4h':
        # –î–ª—è 4-—á–∞—Å–æ–≤—ã—Ö –≥—Ä—É–∑–∏–º 4 –≥–æ–¥–∞ (~8.7K —Å–≤–µ—á–µ–π)
        start_date = (datetime.now() - timedelta(days=1460)).strftime('%Y-%m-%d')
        print(f"üì• –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ 4-—á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {normalized_symbol} —Å {start_date}")
    else:
        # –î–ª—è –¥–Ω–µ–≤–Ω—ã—Ö –∏ –Ω–µ–¥–µ–ª—å–Ω—ã—Ö –≥—Ä—É–∑–∏–º —Å 2017 –≥–æ–¥–∞
        start_date = "2017-01-01"
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö {normalized_symbol} {timeframe} —Å {start_date}")
    
    df = fetch_binance_ohlcv_fast(start_date, end_date, timeframe, normalized_symbol)
    
    if not df.empty:
        df['date'] = df['date'].dt.tz_localize(None)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞–Ω–Ω—ã—Ö
        earliest = df['date'].min()
        latest = df['date'].max()
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω: {earliest.date()} - {latest.date()} ({len(df)} —Å–≤–µ—á–µ–π)")
        
        save_to_cache(cache_key, df)
    
    return df

def build_features_fast(df: pd.DataFrame, timeframe: str = '1d') -> pd.DataFrame:
    """–°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞—Å—á–µ—Ç–æ–º –æ–±—ä–µ–º–∞ –ø–æ –¢–ó"""
    if df.empty:
        return pd.DataFrame()
    
    df = df.copy()
    
    # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
    hl_range = np.maximum(df['high'] - df['low'], 1e-12)
    
    # –ë—ã—Å—Ç—Ä—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    body = (df['close'] - df['open']) / hl_range
    upper = (df['high'] - np.maximum(df['open'], df['close'])) / hl_range
    lower = (np.minimum(df['open'], df['close']) - df['low']) / hl_range
    
    # –†–∞—Å—á–µ—Ç –æ–±—ä–µ–º–∞ –ø–æ –¢–ó - —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–∞ 30 –ø–µ—Ä–∏–æ–¥–æ–≤
    vol_ma = df['volume'].rolling(window=30, min_periods=1).mean()
    vol_rel = df['volume'] / np.maximum(vol_ma, 1)
    
    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (1 –¥–ª—è –±—ã—á—å–µ–π, -1 –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–π, 0 –¥–ª—è –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–π)
    direction = np.where(df['close'] > df['open'], 1.0, 
                        np.where(df['close'] < df['open'], -1.0, 0.0))
    
    features = pd.DataFrame({
        'date': df['date'],
        'body': body,
        'upper': upper,
        'lower': lower,
        'vol_rel': vol_rel,
        'direction': direction,
        'close': df['close'],
        'timeframe': timeframe,
        'symbol': df.get('symbol', 'BTCUSDT')
    })
    
    return features

def calculate_single_candle_distance(candle_a, candle_b):
    """–†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–π —Å–≤–µ—á–∏ –ø–æ –¢–ó"""
    # candle_a –∏ candle_b = [body, vol_rel, upper, lower]
    body_diff = abs(candle_a[0] - candle_b[0]) * W_BODY
    vol_diff = abs(candle_a[1] - candle_b[1]) * W_VOL
    upper_diff = abs(candle_a[2] - candle_b[2]) * W_UP
    lower_diff = abs(candle_a[3] - candle_b[3]) * W_LOW
    
    return body_diff + vol_diff + upper_diff + lower_diff

def compare_patterns_correct(pattern_features, candidate_features):
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –ø–æ –¢–ó:
    - –ö–∞–∂–¥–∞—è —Å–≤–µ—á–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: "identical", "similar" –∏–ª–∏ None
    """
    identical_count = 0
    similar_count = 0
    
    for i in range(len(pattern_features)):
        # –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–π —Å–≤–µ—á–∏
        candle_distance = calculate_single_candle_distance(
            pattern_features[i], candidate_features[i])
        
        if candle_distance <= IDENTICAL_THRESHOLD:
            identical_count += 1
        elif candle_distance <= SIMILAR_THRESHOLD:
            similar_count += 1
        else:
            return None, None  # –ï—Å—Ç—å –Ω–µ–ø–æ—Ö–æ–∂–∞—è —Å–≤–µ—á–∞ - –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç
    
    # –ï—Å–ª–∏ –≤—Å–µ —Å–≤–µ—á–∏ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É
    if identical_count == len(pattern_features):
        return "identical", identical_count
    else:
        return "similar", identical_count + similar_count

def calculate_price_changes_with_stats(matched_patterns, ohlcv_df, timeframe, symbol='BTCUSDT', candles_after=1):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã —Å –º–µ–¥–∏–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π, –≤–∫–ª—é—á–∞—è –º–∞–∫—Å/–º–∏–Ω –∑–Ω–∞—á–µ–Ω–∏—è
    """
    price_changes = []
    directions = []
    
    for pattern in matched_patterns:
        if not pattern:
            price_changes.append(0.0)
            directions.append(0)
            continue
            
        # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–≤–µ—á–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        last_pattern_candle = pattern[-1]
        pattern_end_date = last_pattern_candle['date']
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–ª–µ–¥—É—é—â—É—é —Å–≤–µ—á—É –ø–æ—Å–ª–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        future_candles = ohlcv_df[ohlcv_df['date'] > pattern_end_date]
        
        if len(future_candles) >= candles_after:
            future_candle = future_candles.iloc[candles_after - 1]
            
            pattern_close_price = last_pattern_candle['close']
            future_close_price = future_candle['close']
            
            price_change_pct = (future_close_price - pattern_close_price) / pattern_close_price * 100
            rounded_change = round(price_change_pct, 2)
            price_changes.append(rounded_change)
            
            if rounded_change > 0.1:
                directions.append(1)
            elif rounded_change < -0.1:
                directions.append(-1)
            else:
                directions.append(0)
        else:
            price_changes.append(0.0)
            directions.append(0)
    
    # –ú–µ–¥–∏–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –º–∞–∫—Å/–º–∏–Ω –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    stats = calculate_median_statistics(price_changes, directions)
    
    return price_changes, stats

def calculate_median_statistics(price_changes, directions):
    """–†–∞—Å—á–µ—Ç –º–µ–¥–∏–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≤—Å–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ –≥—Ä—É–ø–ø—ã –∏ –º–∞–∫—Å/–º–∏–Ω –∑–Ω–∞—á–µ–Ω–∏—è–º–∏"""
    if not price_changes:
        return {
            'median_change': 0,
            'max_change': 0,
            'min_change': 0,
            'bullish_percentage': 0,
            'bearish_percentage': 0,
            'success_rate': 0,
            'median_bullish': 0,
            'median_bearish': 0,
            'total_patterns': 0,
            'max_bullish': 0,
            'max_bearish': 0
        }
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω—É–ª–∏
    valid_changes = [ch for ch in price_changes if ch != 0]
    valid_directions = [dir for i, dir in enumerate(directions) if price_changes[i] != 0]
    
    if not valid_changes:
        return {
            'median_change': 0,
            'max_change': 0,
            'min_change': 0,
            'bullish_percentage': 0,
            'bearish_percentage': 0,
            'success_rate': 0,
            'median_bullish': 0,
            'median_bearish': 0,
            'total_patterns': len(price_changes),
            'max_bullish': 0,
            'max_bearish': 0
        }
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    median_change = float(np.median(valid_changes))
    max_change = float(np.max(valid_changes))
    min_change = float(np.min(valid_changes))
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
    total = len(valid_directions)
    bullish_count = sum(1 for d in valid_directions if d == 1)
    bearish_count = sum(1 for d in valid_directions if d == -1)
    neutral_count = sum(1 for d in valid_directions if d == 0)
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º–µ–∂–¥—É –±—ã—á—å–∏–º–∏ –∏ –º–µ–¥–≤–µ–∂—å–∏–º–∏
    if neutral_count > 0:
        half_neutral = neutral_count // 2
        bullish_count += half_neutral
        bearish_count += neutral_count - half_neutral
    
    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
    total_non_neutral = bullish_count + bearish_count
    bullish_percentage = round((bullish_count / total_non_neutral) * 100, 1) if total_non_neutral > 0 else 0
    bearish_percentage = round((bearish_count / total_non_neutral) * 100, 1) if total_non_neutral > 0 else 0
    
    # Success rate —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    success_rate = round((bullish_count / total) * 100, 1) if total > 0 else 0
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω –ø–æ –≥—Ä—É–ø–ø–∞–º
    bullish_changes = [valid_changes[i] for i, d in enumerate(valid_directions) if d == 1]
    bearish_changes = [valid_changes[i] for i, d in enumerate(valid_directions) if d == -1]
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ–¥–∏–∞–Ω—ã –∏ –º–∞–∫—Å –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≥—Ä—É–ø–ø
    if len(bullish_changes) >= 3:
        median_bullish = float(np.median(bullish_changes))
        max_bullish = float(np.max(bullish_changes))
    elif len(bullish_changes) > 0:
        median_bullish = float(np.mean(bullish_changes))
        max_bullish = float(np.max(bullish_changes))
    else:
        median_bullish = 0
        max_bullish = 0
    
    if len(bearish_changes) >= 3:
        median_bearish = float(np.median(bearish_changes))
        max_bearish = float(np.min(bearish_changes))  # –î–ª—è –º–µ–¥–≤–µ–∂—å–∏—Ö –±–µ—Ä–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ (—Å–∞–º–æ–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ)
    elif len(bearish_changes) > 0:
        median_bearish = float(np.mean(bearish_changes))
        max_bearish = float(np.min(bearish_changes))
    else:
        median_bearish = 0
        max_bearish = 0
    
    return {
        'median_change': round(median_change, 2),
        'max_change': round(max_change, 2),
        'min_change': round(min_change, 2),
        'bullish_percentage': bullish_percentage,
        'bearish_percentage': bearish_percentage,
        'success_rate': success_rate,
        'median_bullish': round(median_bullish, 2),
        'median_bearish': round(median_bearish, 2),
        'max_bullish': round(max_bullish, 2),
        'max_bearish': round(max_bearish, 2),
        'total_patterns': len(price_changes),
        'valid_patterns': len(valid_changes),
        'bullish_count_actual': len(bullish_changes),
        'bearish_count_actual': len(bearish_changes)
    }

# API —Ñ—É–Ω–∫—Ü–∏–∏
def api_get_ohlcv_data(start_date: Optional[str] = None, end_date: Optional[str] = None, 
                   timeframe: str = '1d', symbol: str = 'BTCUSDT') -> Dict:
    """API —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö OHLCV"""
    normalized_symbol = normalize_symbol(symbol)
    try:
        df = get_ohlcv_data(timeframe, normalized_symbol)
        if df.empty:
            return {"success": False, "message": f"Failed to load data for {normalized_symbol}"}
        
        if start_date:
            start_dt = pd.to_datetime(start_date)
            if start_dt.tz is not None:
                start_dt = start_dt.tz_localize(None)
            df = df[df['date'] >= start_dt]
        if end_date:
            end_dt = pd.to_datetime(end_date)
            if end_dt.tz is not None:
                end_dt = end_dt.tz_localize(None)
            df = df[df['date'] <= end_dt]
            
        records = []
        for _, row in df.iterrows():
            open_time = row['date']
            
            time_format = '%Y-%m-%dT%H:%M:%SZ'
            
            if timeframe == '1h':
                close_time = open_time + timedelta(hours=1)
            elif timeframe == '4h':
                close_time = open_time + timedelta(hours=4)
            elif timeframe == '1d':
                close_time = open_time + timedelta(days=1)
            elif timeframe == '1w':
                close_time = open_time + timedelta(weeks=1)
            else:
                close_time = open_time + timedelta(days=1)
            
            records.append({
                'open_time': open_time.strftime(time_format),
                'close_time': close_time.strftime(time_format),
                'open_price': float(row['open']),
                'close_price': float(row['close']),
                'high': float(row['high']),
                'low': float(row['low']),
                'volume': float(row['volume']),
                'timeframe': timeframe,
                'symbol': normalized_symbol
            })
        
        return {
            "success": True, 
            "candles": records,
            "timeframe": timeframe,
            "symbol": normalized_symbol,
            "total_candles": len(records)
        }
        
    except Exception as e:
        return {"success": False, "message": str(e)}

def api_get_data_bounds(timeframe: str = '1d', symbol: str = 'BTCUSDT') -> Dict:
    """API —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü –¥–∞–Ω–Ω—ã—Ö"""
    normalized_symbol = normalize_symbol(symbol)
    try:
        df = get_ohlcv_data(timeframe, normalized_symbol)
        if df.empty:
            today = datetime.now().date()
            start_date = (datetime.now() - timedelta(days=365*8)).date()
            return {
                'success': True, 
                'start': str(start_date), 
                'end': str(today),
                'timeframe': timeframe,
                'symbol': normalized_symbol
            }
        
        start = df['date'].min().date()
        end = df['date'].max().date()
        return {
            'success': True, 
            'start': str(start), 
            'end': str(end),
            'timeframe': timeframe,
            'symbol': normalized_symbol
        }
    except Exception as e:
        return {'success': False, 'message': str(e)}

def get_supported_symbols() -> Dict:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä"""
    return {
        "success": True,
        "symbols": list(SYMBOL_MAP.keys()),
        "binance_symbols": list(SYMBOL_MAP.values())
    }

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def get_full_historical_data(timeframe: str = '1d', symbol: str = 'BTCUSDT') -> pd.DataFrame:
    return get_ohlcv_data(timeframe, symbol)

def fetch_binance_ohlcv(start_date: str, end_date: str, timeframe: str = '1d', symbol: str = 'BTCUSDT') -> pd.DataFrame:
    return fetch_binance_ohlcv_fast(start_date, end_date, timeframe, symbol)

def build_features(df: pd.DataFrame, timeframe: str = '1d') -> pd.DataFrame:
    return build_features_fast(df, timeframe)

# main.py - –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π

def check_data_updates(symbol: str, timeframe: str, last_known_date: str) -> Dict:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏"""
    normalized_symbol = normalize_symbol(symbol)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        full_data = get_ohlcv_data(timeframe, normalized_symbol)
        
        if full_data.empty:
            return {'has_updates': False, 'message': 'No data available'}
        
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∞—Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∏ –±–µ–∑ —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å–∞
        full_data = full_data.copy()
        full_data['date'] = pd.to_datetime(full_data['date']).dt.tz_localize(None)
        
        latest_date = full_data['date'].max()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º last_known_date –∏ —É–±–∏—Ä–∞–µ–º —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å
        last_known_dt = pd.to_datetime(last_known_date)
        if last_known_dt.tz is not None:
            last_known_dt = last_known_dt.tz_localize(None)
        
        print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: last_known={last_known_dt} (tz: {last_known_dt.tz}), latest_in_db={latest_date} (tz: {latest_date.tz})")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–µ–µ last_known_date
        newer_data = full_data[full_data['date'] > last_known_dt]
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(newer_data)}")
        
        if newer_data.empty:
            return {
                'has_updates': False,
                'latest_date': latest_date.strftime('%Y-%m-%dT%H:%M:%SZ'),
                'message': 'Data is up to date'
            }
        
        return {
            'has_updates': True,
            'latest_date': latest_date.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'new_candles_count': len(newer_data),
            'last_known_date': last_known_date,
            'symbol': normalized_symbol,
            'timeframe': timeframe
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ check_data_updates: {str(e)}")
        import traceback
        print(f"üìã Traceback: {traceback.format_exc()}")
        return {'has_updates': False, 'message': f'Error checking updates: {str(e)}'}


def get_latest_ohlcv(symbol: str, timeframe: str, last_known_date: str) -> pd.DataFrame:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –∏–∑–≤–µ—Å—Ç–Ω–æ–π –¥–∞—Ç—ã"""
    normalized_symbol = normalize_symbol(symbol)
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –≤ 3 –ø–µ—Ä–∏–æ–¥–∞ —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏
        if timeframe == '1h':
            margin = timedelta(hours=3)
        elif timeframe == '4h':
            margin = timedelta(hours=12)
        elif timeframe == '1d':
            margin = timedelta(days=3)
        elif timeframe == '1w':
            margin = timedelta(weeks=2)
        else:
            margin = timedelta(days=3)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º last_known_date –∏ —É–±–∏—Ä–∞–µ–º —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤
        last_known_dt = pd.to_datetime(last_known_date)
        if last_known_dt.tz is not None:
            last_known_dt = last_known_dt.tz_localize(None)
        
        start_date = (last_known_dt - margin).strftime('%Y-%m-%d')
        end_date = (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')
        
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö {normalized_symbol} {timeframe} —Å {start_date} –ø–æ {end_date}")
        
        new_data = fetch_binance_ohlcv_fast(start_date, end_date, timeframe, normalized_symbol)
        
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∞—Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∏ –±–µ–∑ —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å–∞
        if not new_data.empty:
            new_data = new_data.copy()
            new_data['date'] = pd.to_datetime(new_data['date']).dt.tz_localize(None)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–µ–µ last_known_date
            new_data = new_data[new_data['date'] > last_known_dt]
            print(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(new_data)} –Ω–æ–≤—ã—Ö —Å–≤–µ—á–µ–π")
        
        return new_data
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        print(f"üìã Traceback: {traceback.format_exc()}")
        return pd.DataFrame()

def merge_ohlcv_data(existing_df: pd.DataFrame, new_df: pd.DataFrame) -> pd.DataFrame:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –Ω–æ–≤—ã–º–∏, —É–±–∏—Ä–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã"""
    if existing_df.empty:
        return new_df
    if new_df.empty:
        return existing_df
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –¥–∞—Ç–µ
    combined = pd.concat([existing_df, new_df], ignore_index=True)
    combined = combined.sort_values('date').drop_duplicates(subset=['date'], keep='last')
    
    return combined.reset_index(drop=True)

# main.py
def get_latest_candle(symbol: str, timeframe: str) -> Optional[Dict]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∞–º—É—é –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–≤–µ—á—É –¥–ª—è —Å–∏–º–≤–æ–ª–∞"""
    try:
        normalized_symbol = normalize_symbol(symbol)
        df = get_ohlcv_data(timeframe, normalized_symbol)
        
        if df.empty:
            return None
            
        latest = df.iloc[-1]
        
        return {
            'open_time': latest['date'].strftime('%Y-%m-%dT%H:%M:%SZ'),
            'open_price': float(latest['open']),
            'close_price': float(latest['close']),
            'high': float(latest['high']),
            'low': float(latest['low']),
            'volume': float(latest['volume']),
            'symbol': normalized_symbol,
            'timeframe': timeframe
        }
    except Exception as e:
        print(f"Error getting latest candle: {e}")
        return None
    
def check_data_freshness(symbol: str, timeframe: str) -> Dict:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–≤–µ–∂–µ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    normalized_symbol = normalize_symbol(symbol)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        current_data = get_ohlcv_data(timeframe, normalized_symbol)
        
        if current_data.empty:
            print(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {normalized_symbol}, –∑–∞–≥—Ä—É–∂–∞–µ–º –∑–∞–Ω–æ–≤–æ...")
            return {'needs_update': True, 'reason': 'No data available'}
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –≤ –¥–∞–Ω–Ω—ã—Ö
        latest_date = current_data['date'].max()
        now = datetime.now()
        
        print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–µ–∂–µ—Å—Ç–∏: {normalized_symbol} {timeframe}")
        print(f"üìÖ –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –≤ –¥–∞–Ω–Ω—ã—Ö: {latest_date}")
        print(f"‚è∞ –¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {now}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–π –≤–æ–∑—Ä–∞—Å—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        if timeframe == '1h':
            max_age = timedelta(hours=2)  # 2 —á–∞—Å–∞ –¥–ª—è —á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        elif timeframe == '4h':
            max_age = timedelta(hours=6)  # 6 —á–∞—Å–æ–≤ –¥–ª—è 4-—á–∞—Å–æ–≤—ã—Ö
        elif timeframe == '1d':
            max_age = timedelta(days=1)   # 1 –¥–µ–Ω—å –¥–ª—è –¥–Ω–µ–≤–Ω—ã—Ö
        elif timeframe == '1w':
            max_age = timedelta(days=3)   # 3 –¥–Ω—è –¥–ª—è –Ω–µ–¥–µ–ª—å–Ω—ã—Ö
        else:
            max_age = timedelta(days=1)   # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1 –¥–µ–Ω—å
        
        data_age = now - latest_date
        print(f"‚è≥ –í–æ–∑—Ä–∞—Å—Ç –¥–∞–Ω–Ω—ã—Ö: {data_age}")
        
        if data_age > max_age:
            print(f"üîÑ –î–∞–Ω–Ω—ã–µ —É—Å—Ç–∞—Ä–µ–ª–∏ (–≤–æ–∑—Ä–∞—Å—Ç: {data_age}), —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ")
            return {
                'needs_update': True, 
                'reason': f'Data is {data_age} old, max allowed is {max_age}',
                'latest_date': latest_date.strftime('%Y-%m-%dT%H:%M:%SZ'),
                'current_time': now.strftime('%Y-%m-%dT%H:%M:%SZ')
            }
        else:
            print("‚úÖ –î–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã")
            return {
                'needs_update': False,
                'latest_date': latest_date.strftime('%Y-%m-%dT%H:%M:%SZ'),
                'data_age_hours': round(data_age.total_seconds() / 3600, 1)
            }
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–≤–µ–∂–µ—Å—Ç–∏: {e}")
        return {'needs_update': True, 'reason': f'Error: {str(e)}'}
    
# main.py - –¥–æ–±–∞–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–¥–≥—Ä—É–∑–∫–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

def extend_data_for_patterns(matched_patterns: List[List[Dict]], ohlcv_df: pd.DataFrame, 
                           timeframe: str, symbol: str) -> pd.DataFrame:
    """–ü–æ–¥–≥—Ä—É–∂–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    if not matched_patterns:
        return ohlcv_df
    
    normalized_symbol = normalize_symbol(symbol)
    extended_dfs = [ohlcv_df]
    
    for pattern in matched_patterns:
        if not pattern:
            continue
            
        # –ë–µ—Ä–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é —Å–≤–µ—á—É –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        center_idx = len(pattern) // 2
        center_date = pattern[center_idx]['date']
        
        # –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤–æ–∫—Ä—É–≥ —ç—Ç–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        extended_df = fetch_additional_data_around_date(center_date, timeframe, normalized_symbol)
        if not extended_df.empty:
            extended_dfs.append(extended_df)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ, —É–±–∏—Ä–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã
    if len(extended_dfs) > 1:
        combined_df = pd.concat(extended_dfs, ignore_index=True)
        combined_df = combined_df.drop_duplicates(subset=['date']).sort_values('date').reset_index(drop=True)
        print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(ohlcv_df)} + {sum(len(df) for df in extended_dfs[1:])} -> {len(combined_df)} —Å–≤–µ—á–µ–π")
        return combined_df
    
    return ohlcv_df

def fetch_additional_data_around_date(center_date: datetime, timeframe: str, symbol: str, 
                                    days_before: int = 60, days_after: int = 60) -> pd.DataFrame:
    """–ü–æ–¥–≥—Ä—É–∂–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–æ–∫—Ä—É–≥ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã"""
    cache_key = f"extended_{symbol}_{timeframe}_{center_date.strftime('%Y%m%d')}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached = load_from_cache(cache_key)
    if cached is not None:
        return cached
    
    print(f"üì• –ü–æ–¥–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤–æ–∫—Ä—É–≥ {center_date.date()}")
    
    start_date = (center_date - timedelta(days=days_before)).strftime('%Y-%m-%d')
    end_date = (center_date + timedelta(days=days_after)).strftime('%Y-%m-%d')
    
    extended_df = fetch_binance_ohlcv_fast(start_date, end_date, timeframe, symbol)
    
    # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    save_to_cache(cache_key, extended_df)
    
    return extended_df

# –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∞–Ω–∞–ª–∏–∑–∞ (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è)
def analyze_selected_pattern(selected_candles: List[Dict], num_candles: int, timeframe: str = '1d', 
                           symbol: str = 'BTCUSDT', no_cache: bool = False) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –ø–æ–¥–≥—Ä—É–∑–∫–æ–π –¥–∞–Ω–Ω—ã—Ö"""
    normalized_symbol = normalize_symbol(symbol)
    
    try:
        if not selected_candles or len(selected_candles) != num_candles:
            return {"error": f"Number of candles mismatch: expected {num_candles}, got {len(selected_candles)}"}
        
        print(f"–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞: {len(selected_candles)} —Å–≤–µ—á–µ–π, –ü–∞—Ä–∞: {normalized_symbol}, –¢–§: {timeframe}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ë–ê–ó–û–í–´–ï –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
        ohlcv_df = get_ohlcv_data(timeframe, normalized_symbol)
        if ohlcv_df.empty:
            return {"error": f"Failed to load historical data for {normalized_symbol}"}
        
        print(f"üìä –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(ohlcv_df)} —Å–≤–µ—á–µ–π")
        
        # –°—Ç—Ä–æ–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –±–∞–∑–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        features_df = build_features_fast(ohlcv_df, timeframe)
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—ã –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π
        selected_dates = []
        for candle in selected_candles:
            try:
                open_time = pd.to_datetime(candle['open_time'])
                if open_time.tz is not None:
                    open_time = open_time.tz_localize(None)
                if timeframe == '1d':
                    open_time = open_time.normalize()
                selected_dates.append(open_time)
            except Exception as e:
                return {"error": f"Invalid date format: {candle['open_time']}"}
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å–≤–µ—á–µ–π –≤ features_df
        pat_idx = []
        for selected_date in selected_dates:
            time_diff = np.abs(features_df['date'] - selected_date)
            closest_idx = time_diff.idxmin()
            closest_date = features_df.loc[closest_idx, 'date']
            closest_diff = time_diff.loc[closest_idx]
            
            max_diff = pd.Timedelta(days=1) if timeframe == '1d' else pd.Timedelta(hours=1)
            if closest_diff <= max_diff:
                pat_idx.append(closest_idx)
            else:
                return {"error": f"Could not find matching candle for date {selected_date}"}
        
        if len(pat_idx) != num_candles:
            return {"error": f"Could not find all selected candles in historical data"}
        
        pat_idx.sort()
        
        # –ü–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        pattern_matrix = features_df.loc[pat_idx, ['body', 'vol_rel', 'upper', 'lower']].values
        
        # –ò—â–µ–º –í–û –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö –¥–æ –Ω–∞—á–∞–ª–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–ö–ê–ö –†–ê–ù–¨–®–ï)
        pattern_start_date = features_df.loc[pat_idx[0], 'date']
        search_mask = (features_df['date'] < pattern_start_date)
        search_indices = features_df.index[search_mask].tolist()
        
        feature_matrix = features_df[['body', 'vol_rel', 'upper', 'lower']].values
        
        identical_matches = []
        similar_matches = []
        matched_patterns = []
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –æ–∫–æ–Ω –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ (–ö–ê–ö –†–ê–ù–¨–®–ï)
        max_windows = min(1500, len(search_indices))
        step = max(1, len(search_indices) // max_windows)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ –æ–∫–Ω–æ-–∫–∞–Ω–¥–∏–¥–∞—Ç (–ö–ê–ö –†–ê–ù–¨–®–ï)
        for i in search_indices[::step]:
            j = i + num_candles - 1
            if j >= len(features_df):
                continue
                
            candidate_features = feature_matrix[i:j+1]
            match_type, _ = compare_patterns_correct(pattern_matrix, candidate_features)
            
            if match_type == "identical":
                identical_matches.append((i, j))
            elif match_type == "similar":
                similar_matches.append((i, j))
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        all_matches = identical_matches + similar_matches
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (–ö–ê–ö –†–ê–ù–¨–®–ï)
        for match in all_matches:
            i, j = match
            pattern_data = []
            for k in range(num_candles):
                candle_idx = i + k
                candle_date = features_df.loc[candle_idx, 'date']
                ohlcv_row = ohlcv_df[ohlcv_df['date'] == candle_date].iloc[0]
                
                pattern_data.append({
                    'date': candle_date,
                    'open': float(ohlcv_row['open']),
                    'high': float(ohlcv_row['high']),
                    'low': float(ohlcv_row['low']),
                    'close': float(ohlcv_row['close']),
                    'volume': float(ohlcv_row['volume']),
                    'direction': 'bullish' if features_df.loc[candle_idx, 'direction'] > 0 else 'bearish',
                    'timeframe': timeframe,
                    'symbol': normalized_symbol
                })
            
            matched_patterns.append(pattern_data)

        print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö: {len(identical_matches)}, –ø–æ—Ö–æ–∂–∏—Ö: {len(similar_matches)})")
        
        # üî• –ù–û–í–û–ï: –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if matched_patterns:
            print("üì• –ü–æ–¥–≥—Ä—É–∑–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")
            extended_ohlcv_df = extend_data_for_patterns(matched_patterns, ohlcv_df, timeframe, normalized_symbol)
            
            # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            if len(extended_ohlcv_df) > len(ohlcv_df):
                print(f"üîÑ –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(extended_ohlcv_df)} —Å–≤–µ—á–µ–π")
                extended_features_df = build_features_fast(extended_ohlcv_df, timeframe)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º ohlcv_df –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ü–µ–Ω—ã
                ohlcv_df = extended_ohlcv_df
            else:
                extended_features_df = features_df
        else:
            extended_features_df = features_df
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
        price_changes, performance_stats = calculate_price_changes_with_stats(matched_patterns, ohlcv_df, timeframe, normalized_symbol, candles_after=1)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
        stat_counts = {
            "Identical": len(identical_matches),
            "Similar": len(similar_matches),
            "Total": len(all_matches)
        }
        
        total_found = len(all_matches)
        stat_perc = {
            "Identical": round((len(identical_matches) / total_found * 100) if total_found > 0 else 0, 1),
            "Similar": round((len(similar_matches) / total_found * 100) if total_found > 0 else 0, 1),
            "Total": 100.0 if total_found > 0 else 0
        }
        
        return {
            'success': True,
            'pattern_info': {
                'pattern_start': str(features_df.loc[pat_idx[0], 'date']),
                'pattern_end': str(features_df.loc[pat_idx[-1], 'date']),
                'pattern_len': num_candles,
                'timeframe': timeframe,
                'symbol': normalized_symbol
            },
            'statistics': {
                'matches_found': len(all_matches),
                'identical_count': len(identical_matches),
                'similar_count': len(similar_matches),
                'distribution_counts': stat_counts,
                'distribution_percents': stat_perc
            },
            'matched_patterns': matched_patterns,
            'price_changes': price_changes,
            'performance_stats': performance_stats,
            'data_info': {
                'base_data_points': len(features_df),
                'extended_data_points': len(extended_features_df) if 'extended_features_df' in locals() else len(features_df)
            }
        }
        
    except Exception as e:
        return {"error": f"Analysis error: {str(e)}"}